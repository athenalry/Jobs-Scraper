{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_job_title_from_result(soup): \n",
    "  jobs = []\n",
    "    for div in soup.find_all(name=”div”, attrs={“class”:”row”}):\n",
    "      for a in div.find_all(name=”a”, attrs={“data-tn-element”:”jobTitle”}):\n",
    "      jobs.append(a[“title”])\n",
    "  return(jobs)\n",
    "\n",
    "def extract_company_from_result(soup): \n",
    "  companies = []\n",
    "  for div in soup.find_all(name=”div”, attrs={“class”:”row”}):\n",
    "    company = div.find_all(name=”span”, attrs={“class”:”company”})\n",
    "    if len(company) > 0:\n",
    "      for b in company:\n",
    "        companies.append(b.text.strip())\n",
    "    else:\n",
    "      sec_try = div.find_all(name=”span”, attrs={“class”:”result-link-source”})\n",
    "        for span in sec_try:\n",
    "          companies.append(span.text.strip())\n",
    " return(companies)\n",
    " \n",
    "extract_company_from_result(soup)\n",
    "\n",
    "\n",
    "ch piece of information we want:\n",
    "URL = “https://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "#conducting a request of the stated URL above:\n",
    "page = requests.get(URL)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, “html.parser”)\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "print(soup.prettify())\n",
    "\n",
    "#scraping code:\n",
    "for city in city_set:\n",
    "  for start in range(0, max_results_per_city, 10):\n",
    "  page = requests.get(‘http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=' + str(city) + ‘&start=’ + str(start))\n",
    "  time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "  soup = BeautifulSoup(page.text, “lxml”, from_encoding=”utf-8\")\n",
    "  for div in soup.find_all(name=”div”, attrs={“class”:”row”}): \n",
    "    #specifying row num for index of job posting in dataframe\n",
    "    num = (len(sample_df) + 1) \n",
    "    #creating an empty list to hold the data for each posting\n",
    "    job_post = [] \n",
    "    #append city name\n",
    "    job_post.append(city) \n",
    "    #grabbing job title\n",
    "    for a in div.find_all(name=”a”, attrs={“data-tn-element”:”jobTitle”}):\n",
    "      job_post.append(a[“title”]) \n",
    "    #grabbing company name\n",
    "    company = div.find_all(name=”span”, attrs={“class”:”company”}) \n",
    "    if len(company) > 0: \n",
    "      for b in company:\n",
    "        job_post.append(b.text.strip()) \n",
    "    else: \n",
    "      sec_try = div.find_all(name=”span”, attrs={“class”:”result-link-source”})\n",
    "      for span in sec_try:\n",
    "        job_post.append(span.text) \n",
    "    #grabbing location name\n",
    "    c = div.findAll(‘span’, attrs={‘class’: ‘location’}) \n",
    "    for span in c: \n",
    "      job_post.append(span.text) \n",
    "    #grabbing summary text\n",
    "    d = div.findAll(‘span’, attrs={‘class’: ‘summary’}) \n",
    "      for span in d:\n",
    "        job_post.append(span.text.strip()) \n",
    "    #grabbing salary\n",
    "    try:\n",
    "      job_post.append(div.find(‘nobr’).text) \n",
    "    except:\n",
    "      try:\n",
    "        div_two = div.find(name=”div”, attrs={“class”:”sjcl”}) \n",
    "        div_three = div_two.find(“div”) \n",
    "        job_post.append(div_three.text.strip())\n",
    "      except:\n",
    "        job_post.append(“Nothing_found”) \n",
    "    #appending list of job post info to dataframe at index num\n",
    "    sample_df.loc[num] = job_post\n",
    "\n",
    "#saving sample_df as a local csv file — define your own local path to save contents \n",
    "sample_df.to_csv(“[filepath].csv”, encoding=’utf-8')\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
